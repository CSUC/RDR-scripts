{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSUC/RDR-scripts/blob/main/dataset_size_calculator/dataset_size_calculator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script to Calculate Datasets Size of dataverses in Dataverse\n",
        "### OBSERVATION:\n",
        "This script is available in the following GitHub repository: <a href='https://github.com/CSUC/RDR-scripts/tree/main/dataset_size_calculator' target='_blank'>RDR-scripts</a>. </p> If you have questions or doubts about the code, please contact rdr-contacte@csuc.cat.\n",
        "### SCRIPT OBJECTIVE:\n",
        "This script calculates the total size of a datasets hosted in a dataverse in the Research Data Repository (https://dataverse.csuc.cat/). It uses the Dataverse API to retrieve the size of all files associated with all datasets of a dataverse and returns the total size in bytes, KB, MB, or GB.\n"
      ],
      "metadata": {
        "id": "gyMLbpUS8gWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyQh0bc2nm67",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title First click the &#x25B6; button to execute the script. </p> Then, enter the token (If you don't have your API token, you can get it from the following link <a href='https://dataverse.csuc.cat/dataverseuser.xhtml?selectTab=apiTokenTab' target='_blank'>Get API Token</a>).</p> Finally choose one or more institutions.\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Function to install required packages\n",
        "def install_packages():\n",
        "    \"\"\"\n",
        "    Function to install or update necessary Python packages.\n",
        "    \"\"\"\n",
        "    # Upgrade pip first\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"-q\"])\n",
        "\n",
        "    # Install the required libraries\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyDataverse\", \"-q\"])\n",
        "\n",
        "    print(\"Libraries have been downloaded or updated.\")\n",
        "\n",
        "# Install libraries if they are not installed already\n",
        "try:\n",
        "    import pyDataverse\n",
        "except ImportError:\n",
        "    print(\"Installing libraries...\")\n",
        "    install_packages()\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, FileLink\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "from pyDataverse.api import NativeApi, DataAccessApi, MetricsApi\n",
        "from pyDataverse.models import Dataverse\n",
        "import pandas as pd\n",
        "import requests\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "class UtilsConnection:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def call_api(self, url, method, data=None):\n",
        "        payload = {}\n",
        "        headers = {'X-Dataverse-key': config.get_token()}\n",
        "\n",
        "        response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, api_url, logger, token):\n",
        "        self.api_url = api_url\n",
        "        self.logger = logger\n",
        "        self.token = token\n",
        "\n",
        "    def get_app_config(self):\n",
        "        return self\n",
        "\n",
        "    def get_api_url(self):\n",
        "        return self.api_url\n",
        "\n",
        "    def get_logger(self):\n",
        "        return self.logger\n",
        "\n",
        "    def get_token(self):\n",
        "        return self.token\n",
        "\n",
        "class DatasetProcessor:\n",
        "    def __init__(self, config, identifier):\n",
        "        self.config = config\n",
        "        self.list_datasets = []\n",
        "        self.list_dataverse_children = []\n",
        "        self.identifier = identifier\n",
        "\n",
        "    def update_list_dataset(self, dataset_id):\n",
        "        self.list_datasets.append(dataset_id)\n",
        "\n",
        "    def update_list_dataverse_children(self, dataseverse_id):\n",
        "        self.list_dataverse_children.append(dataseverse_id)\n",
        "\n",
        "    def remove_id_list_dataverse_children(self, dataseverse_id):\n",
        "        self.list_dataverse_children.remove(dataseverse_id)\n",
        "\n",
        "    def get_list_datasets(self):\n",
        "        return self.list_datasets\n",
        "\n",
        "    def get_list_dataverse_children(self):\n",
        "        return self.list_dataverse_children\n",
        "\n",
        "    def count(self):\n",
        "        return len(self.list_datasets)\n",
        "\n",
        "    def create_list_datasets(self, identifier):\n",
        "\n",
        "        conn = UtilsConnection(self.config)\n",
        "\n",
        "        url_api = f\"{self.config.get_api_url()}/api/dataverses/{identifier}/contents\"\n",
        "        object_json = conn.call_api(url_api, \"GET\")\n",
        "\n",
        "        if object_json:\n",
        "            self.config.get_logger().info(f\"Reading the API values\")\n",
        "            array_json = object_json.get(\"data\", {})\n",
        "\n",
        "            for value in array_json:\n",
        "                if value['type'] == 'dataverse':\n",
        "                    self.update_list_dataverse_children(value['id'])\n",
        "                elif value['type'] == 'dataset' and value['protocol'] == 'doi':\n",
        "                    self.update_list_dataset(value['protocol'] + ':' + value['authority'] + '/' + value['identifier'])\n",
        "        else:\n",
        "            self.config.get_logger().error(f\"Call API ERROR\")\n",
        "\n",
        "        if not identifier == self.identifier:\n",
        "            self.remove_id_list_dataverse_children(identifier)\n",
        "\n",
        "        if len(self.get_list_dataverse_children()) != 0:\n",
        "\n",
        "            self.create_list_datasets(self.get_list_dataverse_children()[0])\n",
        "\n",
        "def filemetadata(base_url, token, doi, filemetadata_keys, filemetadata_values):\n",
        "    \"\"\"\n",
        "    Function to extract metadata for files associated with a dataset identified by its DOI.\n",
        "\n",
        "    Parameters:\n",
        "    - base_url: str. Base URL of the Dataverse instance.\n",
        "    - token: str. API token for authentication.\n",
        "    - doi: str. DOI of the dataset.\n",
        "    - filemetadata_keys: list. List to store file metadata keys.\n",
        "    - filemetadata_values: list. List to store file metadata values.\n",
        "\n",
        "    Returns:\n",
        "    - None. Updates the provided lists with extracted file metadata.\n",
        "    \"\"\"\n",
        "    from pyDataverse.api import NativeApi, DataAccessApi\n",
        "    from pyDataverse.models import Dataverse\n",
        "\n",
        "    # Instantiate API objects for accessing Dataverse\n",
        "    api = NativeApi(base_url, token)\n",
        "    data_api = DataAccessApi(base_url, token)\n",
        "\n",
        "    try:\n",
        "        # Retrieve dataset metadata\n",
        "        if(doi != 'doi:10.34810/data1872'):\n",
        "          dataset = api.get_dataset(doi)\n",
        "          # Iterate through files and extract metadata\n",
        "          for i in range(len(dataset.json()['data']['latestVersion']['files'])):\n",
        "              filemetadata_resp = dataset.json()['data']['latestVersion']['files'][i]['dataFile']\n",
        "              filemetadata_keys_aux = list(filemetadata_resp.keys())\n",
        "              filemetadata_values_aux = list(filemetadata_resp.values())\n",
        "              filemetadata_keys.append(filemetadata_keys_aux)\n",
        "              filemetadata_values.append(filemetadata_values_aux)\n",
        "    except KeyError:\n",
        "        print('There was an error reading metadata for the files of the dataset: ' + doi)\n",
        "\n",
        "# Configuration and execution\n",
        "import logging\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# Once they have the token, ask for it\n",
        "token = input(\"Please enter your API token: \")\n",
        "\n",
        "# Use the token in your configuration\n",
        "config = Config(api_url=\"https://dataverse.csuc.cat/\", logger=logger, token=token)\n",
        "\n",
        "# List of institutions\n",
        "institucions = [\n",
        "    'UB', 'UAB', 'UPC', 'UPF', 'UdG', 'UdL', 'URV', 'UOC', 'UVIC-UCC',\n",
        "    'URL', 'UIC', 'UIB', 'Agrotecnio', 'CED', 'CRAG', 'CREAF', 'CRM', 'CTFC',\n",
        "    'i2CAT', 'I3PT', 'IBEC', 'IBEI', 'ICAC-CERCA', 'ICFO-CERCA','ICIQ', 'ICN2',\n",
        "    'ICRA-CERCA', 'IDIBAPS', 'IDIBELL', 'IDIBGI-CERCA', 'IFAE', 'IJC','IRSantPau','CVC','IRSJD',\n",
        "    'IPHES-CERCA', 'IRBBarcelona-CERCA', 'IRB', 'IRSICAIXA', 'IRTA',\n",
        "    'ISGLOBAL', 'VHIR'\n",
        "]\n",
        "# Add an option for selecting all\n",
        "options_institucions = ['All institutions'] + institucions\n",
        "\n",
        "# Create widgets for instructions and selections\n",
        "instruction_text_institucions = widgets.HTML(\n",
        "    value=\"<b>Choose one or more institutions:</b>\"\n",
        ")\n",
        "\n",
        "# Create the widget for multiple selection of institutions\n",
        "institucions_widget = widgets.SelectMultiple(\n",
        "    options=options_institucions,\n",
        "    value=[],\n",
        "    description='Institutions:',\n",
        "    disabled=False\n",
        ")\n",
        "# Function to save selected institutions and print them\n",
        "opcions = set()\n",
        "def save_selection_institucions(change):\n",
        "    global opcions\n",
        "    selected = set(change['new'])\n",
        "\n",
        "    if 'All institutions' in selected:\n",
        "        opcions = set(institucions)  # Select all institutions if 'Totes les institucions' is chosen\n",
        "    else:\n",
        "        opcions.update(selected)  # Update the set with new selections\n",
        "\n",
        "    print(f\"Institutions: {list(opcions)}\")\n",
        "\n",
        "# Function to clear the selection and reset the institutions widget\n",
        "def restart_selection_institucions(button):\n",
        "    global opcions\n",
        "    institucions_widget.value = []  # Clear the selections in the widget\n",
        "    opcions.clear()  # Clear the global opcions set\n",
        "    print(\"The institution selection has been restored.\")\n",
        "\n",
        "# Observe changes in the widget selection for institutions\n",
        "institucions_widget.observe(save_selection_institucions, names='value')\n",
        "\n",
        "# Create a button to restart the selection for institutions\n",
        "restart_button_institucions = widgets.Button(description=\"Reset institution selection\")\n",
        "restart_button_institucions.on_click(restart_selection_institucions)\n",
        "\n",
        "# Display the widgets and button with instructions\n",
        "\n",
        "display(instruction_text_institucions, institucions_widget, restart_button_institucions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbQQlz92FxAA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Once the institution is selected, click the &#x25B6; button to generate the data.</p> Finally, Clic <strong>Download file</strong> button.\n",
        "\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def get_dataset_sizes(base_url, token,doi):\n",
        "    api = NativeApi(base_url, token)\n",
        "    dataset = api.get_dataset(doi)\n",
        "    filemetadata_keys = []\n",
        "    filemetadata_values = []\n",
        "    filemetadata(base_url, token, doi, filemetadata_keys, filemetadata_values)\n",
        "\n",
        "    if not filemetadata_keys or not filemetadata_values:\n",
        "        return 0, 0  # Retorna 0 si no hay metadatos\n",
        "\n",
        "    def get_index(key_list, key):\n",
        "        return key_list.index(key) if key in key_list else None\n",
        "\n",
        "    def get_size(entry, key_list):\n",
        "        original_index = get_index(key_list, 'originalFileSize')\n",
        "        file_index = get_index(key_list, 'filesize')\n",
        "        if original_index is not None and isinstance(entry[original_index], int):\n",
        "            return entry[original_index]\n",
        "        return entry[file_index] if file_index is not None and isinstance(entry[file_index], int) else 0\n",
        "\n",
        "    sizes = [get_size(entry, filemetadata_keys[i]) for i, entry in enumerate(filemetadata_values)]\n",
        "    total_original_size_bytes = sum(sizes)\n",
        "\n",
        "    filesize_index = get_index(filemetadata_keys[0], 'filesize') if filemetadata_keys else None\n",
        "    total_archival_size_bytes = sum(entry[filesize_index] for entry in filemetadata_values if filesize_index is not None and isinstance(entry[filesize_index], int))\n",
        "\n",
        "    return total_original_size_bytes, total_archival_size_bytes\n",
        "\n",
        "def format_size(size_in_bytes):\n",
        "    units = [\"Bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
        "    size = float(size_in_bytes)\n",
        "    unit_index = 0\n",
        "    while size >= 1024 and unit_index < len(units) - 1:\n",
        "        size /= 1024\n",
        "        unit_index += 1\n",
        "    return f\"{size:.2f}\".replace('.', ','), units[unit_index]  # Replaces dot with comma\n",
        "\n",
        "base_url=\"https://dataverse.csuc.cat/\"\n",
        "data = []\n",
        "for element in opcions:\n",
        "    processor = DatasetProcessor(config, element)\n",
        "    processor.create_list_datasets(element)\n",
        "    sigles=element\n",
        "    for i in processor.get_list_datasets():\n",
        "      original_size, archival_size = get_dataset_sizes(base_url,token,i)\n",
        "      formatted_original, unit_original = format_size(original_size)\n",
        "      formatted_archival, unit_archival = format_size(archival_size)\n",
        "      data.append([i, sigles, original_size, archival_size, float(formatted_original.replace(',', '.')), unit_original, float(formatted_archival.replace(',', '.')), unit_archival])\n",
        "df = pd.DataFrame(data, columns=[\"DOI\",\"Institution\", \"Original Size (Bytes)\", \"Archival Size (Bytes)\",  \"Formatted Original Size\", \"Unit (Original Size)\", \"Formatted Archival Size\", \"Unit (Archival Size)\" ])\n",
        "\n",
        "# Extract the numeric part of the DOI and convert it to int for sorting\n",
        "df['DOI_Number'] = df['DOI'].str.extract(r'data(\\d+)').astype(int)\n",
        "\n",
        "# Sort the DataFrame based on the DOI_Number column\n",
        "df = df.sort_values(by='DOI_Number')\n",
        "\n",
        "# Format the DOI column as 'https://doi.org/10.34810/dataXXX'\n",
        "df['DOI'] = 'https://doi.org/10.34810/data' + df['DOI_Number'].astype(str)\n",
        "\n",
        "# Drop the DOI_Number column (optional)\n",
        "df = df.drop(columns=['DOI_Number'])\n",
        "\n",
        "from google.colab import files\n",
        "# Save the DataFrame to an Excel file\n",
        "excel_filename = 'datasets_sizes.xlsx'\n",
        "df.to_excel(excel_filename, index=False)\n",
        "\n",
        "display(df)\n",
        "def create_download_button(filename):\n",
        "    return HTML(f'<a href=\"/files/{filename}\" download=\"{filename}\"><button>Download file</button></a>')\n",
        "\n",
        "# Display the download button\n",
        "create_download_button(excel_filename)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}