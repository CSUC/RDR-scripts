{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSUC/RDR-scripts/blob/main/upload_files/upload_files_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13318227",
      "metadata": {
        "id": "13318227"
      },
      "source": [
        "# Script for Uploading Files Automatically\n",
        "\n",
        "If you have doubts about the code, contact rdr-contacte@csuc.cat\n",
        "\n",
        "## Script Objective\n",
        "The main objective of this script is to automatically upload files to a dataset with their respective metadata placed in an Excel file.\n",
        "\n",
        "## Script Observation\n",
        "\n",
        "Place the script files and the files to be uploaded to the dataset in the same folder on your computer.\n",
        "\n",
        "### In COLAB\n",
        "Upload the Excel file to the Upload files icon.\n",
        "\n",
        "### To generate the Excel and make the script work correctly, you must follow these requirements:\n",
        "\n",
        "- The first row is the header and must contain the name of the variables in this order:\n",
        "    - File Name\n",
        "    - Description\n",
        "    - File Path\n",
        "    - Tag\n",
        "- Each row corresponds to a file.\n",
        "- The file name (File Name) is the only mandatory metadata.\n",
        "    - It must be written correctly and include its extension.\n",
        "- In case any cell does not contain information, it should be left blank.\n",
        "- In case the metadata contains a number, it should be written within quotes.\n",
        "- In the tags variable (Tag), if multiple tags are desired, they should be written separated by a comma.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0da682a",
      "metadata": {
        "cellView": "form",
        "id": "c0da682a"
      },
      "outputs": [],
      "source": [
        "# @title Enter last digits of DOI, the token and the name of the excel file with the extension .xlsx and the repository URL. Click the execute button cell &#x25B6;\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Function to install required packages\n",
        "def install_packages():\n",
        "    \"\"\"\n",
        "    Function to install or update necessary Python packages.\n",
        "    \"\"\"\n",
        "    # Upgrade pip first\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"-q\"])\n",
        "\n",
        "    # Install the required libraries\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyDataverse\", \"-q\"])\n",
        "\n",
        "    print(\"Libraries have been downloaded or updated.\")\n",
        "\n",
        "# Install libraries if they are not installed already\n",
        "try:\n",
        "    import pyDataverse\n",
        "except ImportError:\n",
        "    print(\"Installing libraries...\")\n",
        "    install_packages()\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, FileLink\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import pandas as pd\n",
        "from pyDataverse.api import NativeApi, DataAccessApi\n",
        "from pyDataverse.models import Dataverse, Datafile\n",
        "from pathlib import Path\n",
        "\n",
        "# Provide input values\n",
        "identifier = \"\"  # @param {type:\"string\"}\n",
        "token = \"\"  # @param {type:\"string\"}\n",
        "excel_file_name = \"\" # @param {type:\"string\"}\n",
        "base_url = 'https://dataverse.csuc.cat/'  # @param {type:\"string\"}\n",
        "doi='doi:10.34810/data'+identifier\n",
        "# Initialize API\n",
        "api = NativeApi(base_url, token)\n",
        "data_api = DataAccessApi(base_url, token)\n",
        "\n",
        "def upload_files(base_url, token, doi, excel_file_name):\n",
        "    \"\"\"\n",
        "    Function to upload files to a dataset based on metadata provided in an Excel file.\n",
        "\n",
        "    Args:\n",
        "    base_url (str): Base URL of the repository.\n",
        "    token (str): API token for authentication.\n",
        "    doi (str): DOI of the dataset.\n",
        "    excel_file_name (str): Name of the Excel file with metadata.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Read metadata from Excel file\n",
        "        files_metadata = pd.read_excel(excel_file_name).to_numpy().tolist()\n",
        "        verifier = True\n",
        "        for i in range(len(files_metadata)):\n",
        "            file_name = files_metadata[i][0]\n",
        "            path = Path(file_name)\n",
        "            if not path.is_file():\n",
        "                print('File not found: ' + file_name)\n",
        "                verifier = False\n",
        "        if verifier:\n",
        "            try:\n",
        "                dataset = api.get_dataset(doi)\n",
        "                for i in range(len(files_metadata)):\n",
        "                    df = Datafile()\n",
        "                    df.set({'pid': doi})\n",
        "                    file_name = files_metadata[i][0]\n",
        "                    df.set({'filename': file_name})\n",
        "                    if type(files_metadata[i][1]) != float:\n",
        "                        file_description = files_metadata[i][1]\n",
        "                        df.set({'description': file_description})\n",
        "                    if type(files_metadata[i][2]) != float:\n",
        "                        file_path = files_metadata[i][2]\n",
        "                        df.set({'directoryLabel': file_path})\n",
        "                    if type(files_metadata[i][3]) != float:\n",
        "                        file_categories = files_metadata[i][3].split(\",\")\n",
        "                        df.set({'categories': file_categories})\n",
        "                    df.get()\n",
        "                    resp = api.upload_datafile(doi, file_name, df.json())\n",
        "                    print('File uploaded: ' + file_name)\n",
        "            except:\n",
        "                print('Incorrect token or DOI not found: ' + doi)\n",
        "        else:\n",
        "            print('No files uploaded. Please modify the file names that are incorrect.')\n",
        "    except FileNotFoundError:\n",
        "        print('Metadata file not found: ' + excel_file_name)\n",
        "\n",
        "# Upload files\n",
        "upload_files(base_url, token, doi, excel_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run to get the size of the dataset. Click the execute button cell &#x25B6;\n",
        "def filemetadata(base_url, token, doi, filemetadata_keys, filemetadata_values):\n",
        "    \"\"\"\n",
        "    Function to extract metadata for files associated with a dataset identified by its DOI.\n",
        "\n",
        "    Parameters:\n",
        "    - base_url: str. Base URL of the Dataverse instance.\n",
        "    - token: str. API token for authentication.\n",
        "    - doi: str. DOI of the dataset.\n",
        "    - filemetadata_keys: list. List to store file metadata keys.\n",
        "    - filemetadata_values: list. List to store file metadata values.\n",
        "\n",
        "    Returns:\n",
        "    - None. Updates the provided lists with extracted file metadata.\n",
        "    \"\"\"\n",
        "    from pyDataverse.api import NativeApi, DataAccessApi\n",
        "    from pyDataverse.models import Dataverse\n",
        "\n",
        "    # Instantiate API objects for accessing Dataverse\n",
        "    api = NativeApi(base_url, token)\n",
        "    data_api = DataAccessApi(base_url, token)\n",
        "\n",
        "    try:\n",
        "        # Retrieve dataset metadata\n",
        "        dataset = api.get_dataset(doi)\n",
        "\n",
        "        # Iterate through files and extract metadata\n",
        "        for i in range(len(dataset.json()['data']['latestVersion']['files'])):\n",
        "            filemetadata_resp = dataset.json()['data']['latestVersion']['files'][i]['dataFile']\n",
        "            filemetadata_keys_aux = list(filemetadata_resp.keys())\n",
        "            filemetadata_values_aux = list(filemetadata_resp.values())\n",
        "            filemetadata_keys.append(filemetadata_keys_aux)\n",
        "            filemetadata_values.append(filemetadata_values_aux)\n",
        "    except KeyError:\n",
        "        print('There was an error reading metadata for the files of the dataset: ' + doi)\n",
        "# Get the file metadata\n",
        "dataset = api.get_dataset(doi)\n",
        "filemetadata_keys=[]\n",
        "filemetadata_values=[]\n",
        "filemetadata(base_url, token, doi, filemetadata_keys, filemetadata_values)\n",
        "def format_size(size_in_bytes):\n",
        "    units = [\"Bytes\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
        "    size = float(size_in_bytes)\n",
        "    unit_index = 0\n",
        "\n",
        "    while size >= 1024 and unit_index < len(units) - 1:\n",
        "        size /= 1024\n",
        "        unit_index += 1\n",
        "\n",
        "    return f\"{size:.2f} {units[unit_index]}\"\n",
        "\n",
        "# Determine indexes dynamically for each entry\n",
        "def get_index(key_list, key):\n",
        "    return key_list.index(key) if key in key_list else None\n",
        "\n",
        "# Get indices for 'filesize' and 'originalFileSize'\n",
        "filesize_index = get_index(filemetadata_keys[0], 'filesize')\n",
        "original_size_index = get_index(filemetadata_keys[-1], 'originalFileSize')\n",
        "\n",
        "def get_size(entry, key_list):\n",
        "    original_index = get_index(key_list, 'originalFileSize')\n",
        "    file_index = get_index(key_list, 'filesize')\n",
        "    if original_index is not None and isinstance(entry[original_index], int):\n",
        "        return entry[original_index]\n",
        "    return entry[file_index]\n",
        "\n",
        "# Compute total sizes\n",
        "sizes = [get_size(entry, filemetadata_keys[i]) for i, entry in enumerate(filemetadata_values)]\n",
        "\n",
        "total_original_size_bytes = sum(sizes)\n",
        "total_archival_size_bytes = sum(entry[filesize_index] for entry in filemetadata_values)\n",
        "\n",
        "# Format and print the results\n",
        "print(\"Total original format dataset size:\", format_size(total_original_size_bytes))\n",
        "print(\"Total archival format dataset size:\", format_size(total_archival_size_bytes))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Gc_tEfBs2voX"
      },
      "id": "Gc_tEfBs2voX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}