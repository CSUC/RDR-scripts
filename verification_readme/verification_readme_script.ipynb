{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script to verify if the Readme file exists\n",
        "### OBSERVATION:\n",
        "This script is available in the following GitHub repository: <a href='https://github.com/CSUC/RDR-scripts/tree/main/related_publication_check' target='_blank'>RDR-scripts</a>. </p> If you have questions or doubts about the code, please contact rdr-contacte@csuc.cat.\n",
        "### SCRIPT OBJECTIVE:\n",
        "The main objective of this script is to verify if the README file is in the datasets of an institution."
      ],
      "metadata": {
        "id": "Wm_MGO6iY9CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title First click the &#x25B6; button to execute the script. </p> Then, enter the token (If you don't have your API token, you can get it from the following link <a href='https://dataverse.csuc.cat/dataverseuser.xhtml?selectTab=apiTokenTab' target='_blank'>Get API Token</a>).</p> After that, enter the LAST DIGITS of the DOI (for example, if the DOI ends in <strong>dataXYZ</strong>, only write the number <strong>XYZ</strong> ).</p> Finally click <strong>Download Readme verification file</strong> to download the file.\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Function to install required packages\n",
        "def install_packages():\n",
        "    \"\"\"\n",
        "    Function to install or update necessary Python packages.\n",
        "    \"\"\"\n",
        "    # Upgrade pip first\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"-q\"])\n",
        "\n",
        "    # Install the required libraries\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyDataverse\", \"-q\"])\n",
        "\n",
        "\n",
        "    print(\"Libraries have been downloaded or updated.\")\n",
        "\n",
        "# Install libraries if they are not installed already\n",
        "try:\n",
        "    import pyDataverse\n",
        "except ImportError:\n",
        "    print(\"Installing libraries...\")\n",
        "    install_packages()\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, FileLink\n",
        "from google.colab import files\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "from pyDataverse.api import NativeApi, DataAccessApi, MetricsApi\n",
        "from pyDataverse.models import Dataverse\n",
        "import pandas as pd\n",
        "import requests\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "class UtilsConnection:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def call_api(self, url, method, data=None):\n",
        "        payload = {}\n",
        "        headers = {'X-Dataverse-key': config.get_token()}\n",
        "\n",
        "        response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, api_url, logger, token):\n",
        "        self.api_url = api_url\n",
        "        self.logger = logger\n",
        "        self.token = token\n",
        "\n",
        "    def get_app_config(self):\n",
        "        return self\n",
        "\n",
        "    def get_api_url(self):\n",
        "        return self.api_url\n",
        "\n",
        "    def get_logger(self):\n",
        "        return self.logger\n",
        "\n",
        "    def get_token(self):\n",
        "        return self.token\n",
        "\n",
        "class DatasetProcessor:\n",
        "    def __init__(self, config, identifier):\n",
        "        self.config = config\n",
        "        self.list_datasets = []\n",
        "        self.list_dataverse_children = []\n",
        "        self.identifier = identifier\n",
        "\n",
        "    def update_list_dataset(self, dataset_id):\n",
        "        self.list_datasets.append(dataset_id)\n",
        "\n",
        "    def update_list_dataverse_children(self, dataseverse_id):\n",
        "        self.list_dataverse_children.append(dataseverse_id)\n",
        "\n",
        "    def remove_id_list_dataverse_children(self, dataseverse_id):\n",
        "        self.list_dataverse_children.remove(dataseverse_id)\n",
        "\n",
        "    def get_list_datasets(self):\n",
        "        return self.list_datasets\n",
        "\n",
        "    def get_list_dataverse_children(self):\n",
        "        return self.list_dataverse_children\n",
        "\n",
        "    def count(self):\n",
        "        return len(self.list_datasets)\n",
        "\n",
        "    def create_list_datasets(self, identifier):\n",
        "\n",
        "        conn = UtilsConnection(self.config)\n",
        "\n",
        "        url_api = f\"{self.config.get_api_url()}/api/dataverses/{identifier}/contents\"\n",
        "        object_json = conn.call_api(url_api, \"GET\")\n",
        "\n",
        "        if object_json:\n",
        "            self.config.get_logger().info(f\"Reading the API values\")\n",
        "            array_json = object_json.get(\"data\", {})\n",
        "\n",
        "            for value in array_json:\n",
        "                if value['type'] == 'dataverse':\n",
        "                    self.update_list_dataverse_children(value['id'])\n",
        "                elif value['type'] == 'dataset' and value['protocol'] == 'doi':\n",
        "                    self.update_list_dataset(value['protocol'] + ':' + value['authority'] + '/' + value['identifier'])\n",
        "        else:\n",
        "            self.config.get_logger().error(f\"Call API ERROR\")\n",
        "\n",
        "        if not identifier == self.identifier:\n",
        "            self.remove_id_list_dataverse_children(identifier)\n",
        "\n",
        "        if len(self.get_list_dataverse_children()) != 0:\n",
        "\n",
        "            self.create_list_datasets(self.get_list_dataverse_children()[0])\n",
        "def export_metadata(base_url, token, doi, data_keys, data_values, stateDataset):\n",
        "    \"\"\"\n",
        "    Function to export metadata from a dataset and store it in respective lists.\n",
        "\n",
        "    Args:\n",
        "    base_url (str): Base URL of the Dataverse repository.\n",
        "    token (str): API token for authentication.\n",
        "    doi (str): DOI of the dataset.\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    from pyDataverse.api import NativeApi, DataAccessApi\n",
        "    from pyDataverse.models import Dataverse\n",
        "    api = NativeApi(base_url, token)  # Function to access the API\n",
        "    data_api = DataAccessApi(base_url, token)  # Function to access data via the API\n",
        "    try:\n",
        "        dataset = api.get_dataset(doi)  # Retrieve dataset metadata\n",
        "\n",
        "      # Extract dataset dates metadata if available\n",
        "        dataset_data = dataset.json()['data']['latestVersion']  # Extract once\n",
        "\n",
        "        if \"publicationDate\" in dataset_data:\n",
        "          data_keys.append(\"publicationDate\")\n",
        "          data_values.append(dataset_data[\"publicationDate\"])\n",
        "\n",
        "    except KeyError or InvalidSchema:\n",
        "      pass\n",
        "\n",
        "def filemetadata(base_url, token, doi, filemetadata_keys, filemetadata_values):\n",
        "    \"\"\"\n",
        "    Function to extract metadata for files associated with a dataset identified by its DOI.\n",
        "\n",
        "    Parameters:\n",
        "    - base_url: str. Base URL of the Dataverse instance.\n",
        "    - token: str. API token for authentication.\n",
        "    - doi: str. DOI of the dataset.\n",
        "    - filemetadata_keys: list. List to store file metadata keys.\n",
        "    - filemetadata_values: list. List to store file metadata values.\n",
        "\n",
        "    Returns:\n",
        "    - None. Updates the provided lists with extracted file metadata.\n",
        "    \"\"\"\n",
        "    from pyDataverse.api import NativeApi, DataAccessApi\n",
        "    from pyDataverse.models import Dataverse\n",
        "\n",
        "    # Instantiate API objects for accessing Dataverse\n",
        "    api = NativeApi(base_url, token)\n",
        "    data_api = DataAccessApi(base_url, token)\n",
        "\n",
        "    try:\n",
        "        # Retrieve dataset metadata\n",
        "        if(doi != 'doi:10.34810/data1872'):\n",
        "          dataset = api.get_dataset(doi)\n",
        "          # Iterate through files and extract metadata\n",
        "          for i in range(len(dataset.json()['data']['latestVersion']['files'])):\n",
        "              filemetadata_resp = dataset.json()['data']['latestVersion']['files'][i]['dataFile']\n",
        "              filemetadata_keys_aux = list(filemetadata_resp.keys())\n",
        "              filemetadata_values_aux = list(filemetadata_resp.values())\n",
        "              filemetadata_keys.append(filemetadata_keys_aux)\n",
        "              filemetadata_values.append(filemetadata_values_aux)\n",
        "    except KeyError:\n",
        "        print('There was an error reading metadata for the files of the dataset: ' + doi)\n",
        "\n",
        "# Configuration and execution\n",
        "import logging\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Ask the user for the token\n",
        "token = input(\"Please enter your API token: \")\n",
        "\n",
        "# The base URL is always fixed, no need to ask the user\n",
        "base_url = \"https://dataverse.csuc.cat/\"\n",
        "\n",
        "# Use the token in your configuration\n",
        "config = Config(api_url=\"https://dataverse.csuc.cat/\", logger=logger, token=token)\n",
        "\n",
        "# List of institutions\n",
        "institucions = [\n",
        "    'UB', 'UAB', 'UPC', 'UPF', 'UdG', 'UdL', 'URV', 'UOC', 'UVIC-UCC',\n",
        "    'URL', 'UIC', 'UIB', 'Agrotecnio', 'CED', 'CRAG', 'CREAF', 'CRM', 'CTFC','CVC',\n",
        "    'i2CAT', 'I3PT', 'IBEC', 'IBEI', 'ICAC-CERCA', 'ICFO-CERCA','ICIQ', 'ICN2',\n",
        "    'ICRA-CERCA', 'IDIBAPS', 'IDIBELL', 'IDIBGI-CERCA', 'IFAE', 'IJC','IRSantPau','CVC','IRSJD',\n",
        "    'IPHES-CERCA', 'IRBBarcelona-CERCA', 'IRB', 'IRSICAIXA', 'IRTA','IRSJD',\n",
        "    'ISGLOBAL', 'VHIR'\n",
        "]\n",
        "\n",
        "# Add an option for selecting all\n",
        "options_institucions = ['All institutions'] + institucions\n",
        "\n",
        "# Create widgets for instructions and selections\n",
        "instruction_text_institucions = widgets.HTML(\n",
        "    value=\"<b>Trieu una o més institucions:</b>\"\n",
        ")\n",
        "\n",
        "# Create the widget for multiple selection of institutions\n",
        "institucions_widget = widgets.SelectMultiple(\n",
        "    options=options_institucions,\n",
        "    value=[],\n",
        "    description='Institutions:',\n",
        "    disabled=False\n",
        ")\n",
        "# Function to save selected institutions and print them\n",
        "opcions = set()\n",
        "def save_selection_institucions(change):\n",
        "    global opcions\n",
        "    selected = set(change['new'])\n",
        "\n",
        "    if 'All institutions' in selected:\n",
        "        opcions = set(institucions)  # Select all institutions if 'All institutions' is chosen\n",
        "    else:\n",
        "        opcions.update(selected)  # Update the set with new selections\n",
        "\n",
        "    print(f\"Institutions: {list(opcions)}\")\n",
        "\n",
        "# Function to clear the selection and reset the institutions widget\n",
        "def restart_selection_institucions(button):\n",
        "    global opcions\n",
        "    institucions_widget.value = []  # Clear the selections in the widget\n",
        "    opcions.clear()  # Clear the global opcions set\n",
        "    print(\"La selecció de la institució s'ha restablert.\")\n",
        "\n",
        "# Observe changes in the widget selection for institutions\n",
        "institucions_widget.observe(save_selection_institucions, names='value')\n",
        "\n",
        "# Create a button to restart the selection for institutions\n",
        "restart_button_institucions = widgets.Button(description=\"Reiniciar la selecció d'institucions\")\n",
        "restart_button_institucions.on_click(restart_selection_institucions)\n",
        "\n",
        "# Display the widgets and button with instructions\n",
        "\n",
        "display(instruction_text_institucions, institucions_widget, restart_button_institucions)"
      ],
      "metadata": {
        "id": "PyzcoKHhssE1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Once the institution is selected, click the &#x25B6; button to generate the data.</p> Finally, Click <strong>Download file</strong> button.\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "filemetadata_keys=[]\n",
        "filemetadata_values=[]\n",
        "list_doi = []\n",
        "instancia = []\n",
        "name_readme_file=[]\n",
        "validation_readme_file=[]\n",
        "published=[]\n",
        "for element in opcions:\n",
        "    processor = DatasetProcessor(config, element)\n",
        "    processor.create_list_datasets(element)\n",
        "    sigles=element\n",
        "    for i in processor.get_list_datasets():\n",
        "        #  Metadata lists:\n",
        "        data_keys=[]\n",
        "        data_values=[]\n",
        "        stateDataset = []\n",
        "        # Exporting metadata\n",
        "        export_metadata(config.get_api_url(), config.get_token(), i, data_keys, data_values, stateDataset)\n",
        "        if len(data_values) != 0:\n",
        "          published.append('Published')\n",
        "        else:\n",
        "          published.append('Draft')\n",
        "        # Creating a DataFrame\n",
        "        filemetadata_keys_aux=[]\n",
        "        filemetadata_values_aux=[]\n",
        "        name_readme_file_aux=[]\n",
        "        filemetadata(base_url, token, i, filemetadata_keys_aux, filemetadata_values_aux)\n",
        "        filemetadata_keys.extend(filemetadata_keys_aux) # Change here: Use extend to add elements to the list\n",
        "        filemetadata_values.extend(filemetadata_values_aux) # Change here: Use extend to add elements to the list\n",
        "        instancia.append(sigles)\n",
        "        list_doi.append(i)\n",
        "        # Iterate through filemetadata extracted for the current dataset\n",
        "        for file_keys, file_values in zip(filemetadata_keys_aux, filemetadata_values_aux):\n",
        "            # Check if 'filename' is in the metadata keys\n",
        "            if 'filename' in file_keys:\n",
        "                # Get the index of 'filename' in the keys list\n",
        "                filename_index = file_keys.index('filename')\n",
        "                # Check if the corresponding value contains 'Readme' or 'readme'\n",
        "                if 'readme' in file_values[filename_index].lower():\n",
        "                    name_readme_file_aux.append(file_values[filename_index])\n",
        "        name_readme_file.append(name_readme_file_aux)\n",
        "        if len(name_readme_file_aux)!=0:\n",
        "          validation_readme_file.append('Yes')\n",
        "        else:\n",
        "          validation_readme_file.append('No')\n",
        "# Create the data dictionary\n",
        "data = {\n",
        "    'DOI': list_doi,\n",
        "    'Published': published,\n",
        "    'Institution': instancia,\n",
        "    'Is there Readme?': validation_readme_file,\n",
        "    'Readme file name': name_readme_file\n",
        "}\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract the numeric part of the DOI and convert it to int for sorting\n",
        "df['DOI_Number'] = df['DOI'].str.extract(r'data(\\d+)').astype(int)\n",
        "\n",
        "# Sort the DataFrame based on the DOI_Number column\n",
        "df = df.sort_values(by='DOI_Number')\n",
        "\n",
        "# Format the DOI column as 'https://doi.org/10.34810/dataXXX'\n",
        "df['DOI'] = 'https://doi.org/10.34810/data' + df['DOI_Number'].astype(str)\n",
        "\n",
        "# Drop the DOI_Number column (optional)\n",
        "df = df.drop(columns=['DOI_Number'])\n",
        "# Drop the []\n",
        "df['Readme file name'] = df['Readme file name'].astype(str).str.strip(\"[]\").str.replace(\"'\", \"\", regex=False)\n",
        "from google.colab import files\n",
        "# Save the DataFrame to an Excel file\n",
        "excel_filename = 'datasets_sizes.xlsx'\n",
        "df.to_excel(excel_filename, index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Running in Google Colab\n",
        "    # Provide a download button\n",
        "    download_button = widgets.Button(description=\"Download file\")\n",
        "    display(download_button)\n",
        "\n",
        "    # Function to be executed when the download button is clicked\n",
        "    def on_download_button_click(b):\n",
        "        # Download the Readme.txt file in Google Colab\n",
        "        files.download(excel_filename)\n",
        "\n",
        "    # Event handler for the download button\n",
        "    download_button.on_click(on_download_button_click)\n",
        "\n",
        "else:\n",
        "    # Running in Jupyter Notebook\n",
        "    # Provide a download link\n",
        "    download_link = FileLink(excel_filename, result_html_prefix=\"Click to download the file: \")\n",
        "    display(download_link)"
      ],
      "metadata": {
        "id": "iXOfFplMt1Qy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}