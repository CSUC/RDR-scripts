{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSUC/RDR-scripts/blob/main/related_publication_check/related_publication_check_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Script for Generating Related Publications Metadata\n",
        "### OBSERVATION:\n",
        "This script is available in the following GitHub repository: <a href='https://github.com/CSUC/RDR-scripts/tree/main/related_publication_check' target='_blank'>RDR-scripts</a>. </p> If you have questions or doubts about the code, please contact rdr-contacte@csuc.cat.\n",
        "### SCRIPT OBJECTIVE:\n",
        "This script allows users to assess and generate a metadata dashboard with the related publications metadata associated with datasets of an institution."
      ],
      "metadata": {
        "id": "MTZfBxnhhhaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title First click the &#x25B6; button to execute the script. </p> Then, enter the token (If you don't have your API token, you can get it from the following link <a href='https://dataverse.csuc.cat/dataverseuser.xhtml?selectTab=apiTokenTab' target='_blank'>Get API Token</a>).</p> Finally choose one or more institutions.\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "# Function to install required packages\n",
        "def install_packages():\n",
        "    \"\"\"\n",
        "    Function to install or update necessary Python packages.\n",
        "    \"\"\"\n",
        "    # Upgrade pip first\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"-q\"])\n",
        "\n",
        "    # Install the required libraries\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pyDataverse\", \"-q\"])\n",
        "\n",
        "    print(\"Libraries have been downloaded or updated.\")\n",
        "\n",
        "# Install libraries if they are not installed already\n",
        "try:\n",
        "    import pyDataverse\n",
        "except ImportError:\n",
        "    print(\"Installing libraries...\")\n",
        "    install_packages()\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "from google.colab import output\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "import smtplib\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "from pyDataverse.api import NativeApi, DataAccessApi, MetricsApi\n",
        "from pyDataverse.models import Dataverse\n",
        "import pandas as pd\n",
        "import requests\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "class UtilsConnection:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def call_api(self, url, method, data=None):\n",
        "        payload = {}\n",
        "        headers = {'X-Dataverse-key': config.get_token()}\n",
        "\n",
        "        response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, api_url, logger, token):\n",
        "        self.api_url = api_url\n",
        "        self.logger = logger\n",
        "        self.token = token\n",
        "\n",
        "    def get_app_config(self):\n",
        "        return self\n",
        "\n",
        "    def get_api_url(self):\n",
        "        return self.api_url\n",
        "\n",
        "    def get_logger(self):\n",
        "        return self.logger\n",
        "\n",
        "    def get_token(self):\n",
        "        return self.token\n",
        "\n",
        "class DatasetProcessor:\n",
        "    def __init__(self, config, identifier):\n",
        "        self.config = config\n",
        "        self.list_datasets = []\n",
        "        self.list_dataverse_children = []\n",
        "        self.identifier = identifier\n",
        "\n",
        "    def update_list_dataset(self, dataset_id):\n",
        "        self.list_datasets.append(dataset_id)\n",
        "\n",
        "    def update_list_dataverse_children(self, dataseverse_id):\n",
        "        self.list_dataverse_children.append(dataseverse_id)\n",
        "\n",
        "    def remove_id_list_dataverse_children(self, dataseverse_id):\n",
        "        self.list_dataverse_children.remove(dataseverse_id)\n",
        "\n",
        "    def get_list_datasets(self):\n",
        "        return self.list_datasets\n",
        "\n",
        "    def get_list_dataverse_children(self):\n",
        "        return self.list_dataverse_children\n",
        "\n",
        "    def count(self):\n",
        "        return len(self.list_datasets)\n",
        "\n",
        "    def create_list_datasets(self, identifier):\n",
        "\n",
        "        conn = UtilsConnection(self.config)\n",
        "\n",
        "        url_api = f\"{self.config.get_api_url()}/api/dataverses/{identifier}/contents\"\n",
        "        object_json = conn.call_api(url_api, \"GET\")\n",
        "\n",
        "        if object_json:\n",
        "            self.config.get_logger().info(f\"Reading the API values\")\n",
        "            array_json = object_json.get(\"data\", {})\n",
        "\n",
        "            for value in array_json:\n",
        "                if value['type'] == 'dataverse':\n",
        "                    self.update_list_dataverse_children(value['id'])\n",
        "                elif value['type'] == 'dataset' and value['protocol'] == 'doi':\n",
        "                    self.update_list_dataset(value['protocol'] + ':' + value['authority'] + '/' + value['identifier'])\n",
        "        else:\n",
        "            self.config.get_logger().error(f\"Call API ERROR\")\n",
        "\n",
        "        if not identifier == self.identifier:\n",
        "            self.remove_id_list_dataverse_children(identifier)\n",
        "\n",
        "        if len(self.get_list_dataverse_children()) != 0:\n",
        "\n",
        "            self.create_list_datasets(self.get_list_dataverse_children()[0])\n",
        "def extract_value(data_dict):\n",
        "    \"\"\"\n",
        "    Function to extract type names and values from a JSON metadata dictionary.\n",
        "\n",
        "    Args:\n",
        "    data_dict (dict): JSON metadata dictionary.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Type names and values extracted from the metadata dictionary.\n",
        "    \"\"\"\n",
        "    if isinstance(data_dict, dict):\n",
        "        type_names = []\n",
        "        values = []\n",
        "        for key, value in data_dict.items():\n",
        "            if key == 'typeName' and 'value' in data_dict:\n",
        "                if isinstance(data_dict['value'], list):\n",
        "                    for v in data_dict['value']:\n",
        "                        type_names.append(data_dict['typeName'])\n",
        "                        values.append(v)\n",
        "                else:\n",
        "                    type_names.append(data_dict['typeName'])\n",
        "                    values.append(data_dict['value'])\n",
        "            elif isinstance(value, dict) and 'typeName' in value and 'value' in value:\n",
        "                type_names.append(value['typeName'])\n",
        "                values.append(value['value'])\n",
        "            elif isinstance(value, str) and key == 'typeName':\n",
        "                type_names.append(value)\n",
        "                values.append(value)\n",
        "            else:\n",
        "                extracted_type_names, extracted_values = extract_value(value)\n",
        "                type_names += extracted_type_names\n",
        "                values += extracted_values\n",
        "        return type_names, values\n",
        "    elif isinstance(data_dict, list):\n",
        "        type_names = []\n",
        "        values = []\n",
        "        for item in data_dict:\n",
        "            extracted_type_names, extracted_values = extract_value(item)\n",
        "            type_names += extracted_type_names\n",
        "            values += extracted_values\n",
        "        return type_names, values\n",
        "    else:\n",
        "        return [], []\n",
        "\n",
        "def export_metadata(base_url, token, doi, citation_keys, citation_values, customUAB_keys, customUAB_values, stateDataset):\n",
        "    \"\"\"\n",
        "    Function to export metadata from a dataset and store it in respective lists.\n",
        "\n",
        "    Args:\n",
        "    base_url (str): Base URL of the Dataverse repository.\n",
        "    token (str): API token for authentication.\n",
        "    doi (str): DOI of the dataset.\n",
        "    citation_keys (list): List to store citation metadata keys.\n",
        "    citation_values (list): List to store citation metadata values.\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    from pyDataverse.api import NativeApi, DataAccessApi\n",
        "    from pyDataverse.models import Dataverse\n",
        "    api = NativeApi(base_url, token)  # Function to access the API\n",
        "    data_api = DataAccessApi(base_url, token)  # Function to access data via the API\n",
        "    try:\n",
        "        dataset = api.get_dataset(doi)  # Retrieve dataset metadata\n",
        "\n",
        "\n",
        "      # Extract citation metadata if available\n",
        "        if 'citation' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
        "            metadata_citation = dataset.json()['data']['latestVersion']['metadataBlocks']['citation']['fields']\n",
        "            citation = extract_value(metadata_citation)\n",
        "            citation_keys.extend(citation[0])\n",
        "            citation_values.extend(citation[1])\n",
        "            for item in metadata_citation:\n",
        "                if isinstance(item['value'], str):\n",
        "                    index_change = citation_keys.index(item['typeName'])\n",
        "                    citation_values[index_change] = item['value']\n",
        "\n",
        "     # Extract Library UAB metadata if available\n",
        "\n",
        "        if 'customUAB' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
        "            metadata_customUAB = dataset.json()['data']['latestVersion']['metadataBlocks']['customUAB']['fields']\n",
        "            customUAB = extract_value(metadata_customUAB)\n",
        "            customUAB_keys.extend(customUAB[0])\n",
        "            customUAB_values.extend(customUAB[1])\n",
        "            for item in metadata_customUAB:\n",
        "                if isinstance(item['value'], str):\n",
        "                    index_change = customUAB_keys.index(item['typeName'])\n",
        "                    customUAB_values[index_change] = item['value']\n",
        "\n",
        "    except KeyError or InvalidSchema:\n",
        "        pass\n",
        "\n",
        "def extract_metadata(data, citation_keys, citation_values, customUAB_keys, customUAB_values):\n",
        "\n",
        "    for key, value in zip(citation_keys, citation_values):\n",
        "        if not isinstance(value, dict):\n",
        "            data.append([key, value])\n",
        "\n",
        "    for key, value in zip(customUAB_keys, customUAB_values):\n",
        "        if not isinstance(value, dict):\n",
        "            data.append([key, value])\n",
        "\n",
        "# Configuration and execution\n",
        "import logging\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Ask the user for the token\n",
        "token = input(\"Please enter your API token: \")\n",
        "\n",
        "# Use the token in your configuration\n",
        "config = Config(api_url=\"https://dataverse.csuc.cat/\", logger=logger, token=token)\n",
        "\n",
        "# List of institutions\n",
        "institucions = [\n",
        "    'UB', 'UAB', 'UPC', 'UPF', 'UdG', 'UdL', 'URV', 'UOC', 'UVIC-UCC',\n",
        "    'URL', 'UIC', 'UIB', 'Agrotecnio', 'CED', 'CRAG', 'CREAF', 'CRM', 'CTFC','CVC',\n",
        "    'i2CAT', 'I3PT', 'IBEC', 'IBEI', 'ICAC-CERCA', 'ICFO-CERCA','ICIQ', 'ICN2',\n",
        "    'ICRA-CERCA', 'IDIBAPS', 'IDIBELL', 'IDIBGI-CERCA', 'IFAE', 'IJC','IRSantPau','CVC','IRSJD',\n",
        "    'IPHES-CERCA', 'IRBBarcelona-CERCA', 'IRB', 'IRSICAIXA', 'IRTA','IRSJD'\n",
        "    'ISGLOBAL', 'VHIR'\n",
        "]\n",
        "# Add an option for selecting all\n",
        "options_institucions = ['All institutions'] + institucions\n",
        "\n",
        "# Create widgets for instructions and selections\n",
        "instruction_text_institucions = widgets.HTML(\n",
        "    value=\"<b>Select one or more institutions:</b>\"\n",
        ")\n",
        "\n",
        "# Create the widget for multiple selection of institutions\n",
        "institucions_widget = widgets.SelectMultiple(\n",
        "    options=options_institucions,\n",
        "    value=[],\n",
        "    description='Institutions:',\n",
        "    disabled=False\n",
        ")\n",
        "# Function to save selected institutions and print them\n",
        "opcions = set()\n",
        "def save_selection_institucions(change):\n",
        "    global opcions\n",
        "    selected = set(change['new'])\n",
        "\n",
        "    if 'All institutions' in selected:\n",
        "        opcions = set(institucions)  # Select all institutions if 'All institutions' is chosen\n",
        "    else:\n",
        "        opcions.update(selected)  # Update the set with new selections\n",
        "\n",
        "    print(f\"Institutions: {list(opcions)}\")\n",
        "\n",
        "# Function to clear the selection and reset the institutions widget\n",
        "def restart_selection_institucions(button):\n",
        "    global opcions\n",
        "    institucions_widget.value = []  # Clear the selections in the widget\n",
        "    opcions.clear()  # Clear the global opcions set\n",
        "    print(\"Institution selection has been reset.\")\n",
        "\n",
        "# Observe changes in the widget selection for institutions\n",
        "institucions_widget.observe(save_selection_institucions, names='value')\n",
        "\n",
        "# Create a button to restart the selection for institutions\n",
        "restart_button_institucions = widgets.Button(description=\"Reset institution selection.\")\n",
        "restart_button_institucions.on_click(restart_selection_institucions)\n",
        "\n",
        "# Display the widgets and button with instructions\n",
        "\n",
        "display(instruction_text_institucions, institucions_widget, restart_button_institucions)\n",
        "\n",
        "\n",
        "########################\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YwdSslz8k2Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Once the institution is selected, click the &#x25B6; button to generate the data.</p> Finally, Click <strong>Download file</strong> button.\n",
        "selected_metadata = [\"publicationRelationType\",\"publicationCitation\",\"publicationIDType\",\"publicationIDNumber\",\"publicationURL\",\"reviewLibrary\"]\n",
        "metadata_keys_list = []\n",
        "metadata_values_list = []\n",
        "list_doi = []\n",
        "instancia = []\n",
        "states = []\n",
        "for element in opcions:\n",
        "    processor = DatasetProcessor(config, element)\n",
        "    processor.create_list_datasets(element)\n",
        "    sigles=element\n",
        "    for i in processor.get_list_datasets():\n",
        "        metadata_keys_aux=[]\n",
        "        metadata_values_aux=[]\n",
        "        #  Metadata lists:\n",
        "        citation_keys, customUAB_keys, state = [[] for _ in range(3)]\n",
        "        citation_values, customUAB_values, state = [[] for _ in range(3)]\n",
        "        data = []\n",
        "        stateDataset = []\n",
        "        # Exporting metadata\n",
        "        export_metadata(config.get_api_url(), config.get_token(), i, citation_keys, citation_values, customUAB_keys, customUAB_values, stateDataset)\n",
        "        # Extracting metadata and arranging it\n",
        "        extract_metadata(data, citation_keys, citation_values, customUAB_keys, customUAB_values)\n",
        "        # Creating a DataFrame\n",
        "        df = pd.DataFrame(data, columns=['Metadata', 'Value'])\n",
        "        metadata_keys_aux = df['Metadata'].tolist()\n",
        "        metadata_values_aux = df['Value'].tolist()\n",
        "        metadata_keys_list.append(metadata_keys_aux)\n",
        "        metadata_values_list.append(metadata_values_aux)\n",
        "        instancia.append(sigles)\n",
        "        list_doi.append(i)\n",
        "        #states.append(stateDataset[0])\n",
        "def aggregate_metadata(metadata_keys_list, metadata_values_list, list_doi, selected_metadata):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Initialize dictionaries to store aggregated values\n",
        "    metadata_values = defaultdict(lambda: defaultdict(set))\n",
        "\n",
        "    # Aggregate values by DOI\n",
        "    for i in range(len(metadata_keys_list)):\n",
        "        doi = list_doi[i]\n",
        "        for key, value in zip(metadata_keys_list[i], metadata_values_list[i]):\n",
        "            if key in selected_metadata:\n",
        "                if isinstance(value, list):\n",
        "                    metadata_values[key][doi].update(value)\n",
        "                else:\n",
        "                    metadata_values[key][doi].add(value)\n",
        "\n",
        "    # Convert sets to sorted lists\n",
        "    aggregated_metadata = {field: [''] * len(list_doi) for field in selected_metadata}\n",
        "    for field in selected_metadata:\n",
        "        for doi in list_doi:\n",
        "            values = list(metadata_values[field][doi])\n",
        "            aggregated_metadata[field][list_doi.index(doi)] = '; '.join(values) if values else ''\n",
        "\n",
        "    return aggregated_metadata\n",
        "# Aggregate metadata values\n",
        "metadata = aggregate_metadata(metadata_keys_list, metadata_values_list, list_doi, selected_metadata)\n",
        "\n",
        "# Create the data dictionary\n",
        "data = {\n",
        "    'DOI': list_doi,\n",
        "    'Institution': instancia\n",
        "}\n",
        "\n",
        "# Add the dynamic metadata fields\n",
        "for field in selected_metadata:\n",
        "    data[field] = metadata[field]\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract the numeric part of the DOI and convert it to int for sorting\n",
        "df['DOI_Number'] = df['DOI'].str.extract(r'data(\\d+)').astype(int)\n",
        "\n",
        "# Sort the DataFrame based on the DOI_Number column\n",
        "df = df.sort_values(by='DOI_Number')\n",
        "\n",
        "# Format the DOI column as 'https://doi.org/10.34810/dataXXX'\n",
        "df['DOI'] = 'https://doi.org/10.34810/data' + df['DOI_Number'].astype(str)\n",
        "\n",
        "# Drop the DOI_Number column (optional)\n",
        "df = df.drop(columns=['DOI_Number'])\n",
        "\n",
        "from google.colab import files\n",
        "# Save the DataFrame to an Excel file\n",
        "excel_filename = 'related_publication_metadata.xlsx'\n",
        "df.to_excel(excel_filename, index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df)\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Running in Google Colab\n",
        "    # Provide a download button\n",
        "    download_button = widgets.Button(description=\"Download file\")\n",
        "    display(download_button)\n",
        "\n",
        "    # Function to be executed when the download button is clicked\n",
        "    def on_download_button_click(b):\n",
        "        # Download the Readme.txt file in Google Colab\n",
        "        files.download(excel_filename)\n",
        "\n",
        "    # Event handler for the download button\n",
        "    download_button.on_click(on_download_button_click)\n",
        "\n",
        "else:\n",
        "    # Running in Jupyter Notebook\n",
        "    # Provide a download link\n",
        "    download_link = FileLink(excel_filename, result_html_prefix=\"Click to download the file: \")\n",
        "    display(download_link)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BO6eCyxGJlpj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}