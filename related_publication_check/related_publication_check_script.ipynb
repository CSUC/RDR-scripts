{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Script per a la Generació de Metadades de Publicacions Relacionades\n",
        "# OBSERVACIÓ:\n",
        "Si teniu dubtes sobre el codi, poseu-vos en contacte amb rdr-contacte@csuc.cat\n",
        "## OBJECTIU DE L'SCRIPT\n",
        "Aquest script permet avaluar i generar un tauler de metadades relacionades amb les publicacions associades a conjunts de dades. L'objectiu és extreure i agregar metadades específiques de cada conjunt de dades per a la seva posterior visualització i anàlisi en un fitxer Excel."
      ],
      "metadata": {
        "id": "MTZfBxnhhhaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZmqauwVchbDJ"
      },
      "outputs": [],
      "source": [
        "# @title Instal·leu o actualitzeu les llibreries (feu clic al botó Executar &#x25B6; )\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "def install_packages(b):\n",
        "    \"\"\"\n",
        "    Function to install or update required Python packages.\n",
        "\n",
        "    Args:\n",
        "    b (widget): Button widget that triggers the installation process.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    clear_output(wait=True)\n",
        "    !pip install --upgrade pip -q  # Upgrade pip silently\n",
        "    !pip install pyDataverse -q    # Install or update pyDataverse silently\n",
        "    !pip install requests -q       # Install or update requests silently\n",
        "    print(\"Les llibreries s'han descarregat o actualitzat correctament.\")\n",
        "\n",
        "# Displaying installation message\n",
        "display(HTML(\"<p style='font-size:14px;'><b>Feu clic al botó següent per instal·lar les llibreries necessàries.</b></p>\"))\n",
        "\n",
        "# Creating installation button\n",
        "install_button = widgets.Button(description='Instal·lar llibreries')\n",
        "install_button.on_click(install_packages)\n",
        "\n",
        "# Displaying the installation button\n",
        "display(install_button)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Feu clic al botó Executar &#x25B6; , ompliu el token i trieu les institucions i les metadades\n",
        "import subprocess\n",
        "import smtplib\n",
        "import os\n",
        "from email.mime.multipart import MIMEMultipart\n",
        "from email.mime.base import MIMEBase\n",
        "from email import encoders\n",
        "from pyDataverse.api import NativeApi, DataAccessApi, MetricsApi\n",
        "from pyDataverse.models import Dataverse\n",
        "import pandas as pd\n",
        "import requests\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "class UtilsConnection:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def call_api(self, url, method, data=None):\n",
        "        payload = {}\n",
        "        headers = {'X-Dataverse-key': config.get_token()}\n",
        "\n",
        "        response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, api_url, logger, token):\n",
        "        self.api_url = api_url\n",
        "        self.logger = logger\n",
        "        self.token = token\n",
        "\n",
        "    def get_app_config(self):\n",
        "        return self\n",
        "\n",
        "    def get_api_url(self):\n",
        "        return self.api_url\n",
        "\n",
        "    def get_logger(self):\n",
        "        return self.logger\n",
        "\n",
        "    def get_token(self):\n",
        "        return self.token\n",
        "\n",
        "class DatasetProcessor:\n",
        "    def __init__(self, config, identifier):\n",
        "        self.config = config\n",
        "        self.list_datasets = []\n",
        "        self.list_dataverse_children = []\n",
        "        self.identifier = identifier\n",
        "\n",
        "    def update_list_dataset(self, dataset_id):\n",
        "        self.list_datasets.append(dataset_id)\n",
        "\n",
        "    def update_list_dataverse_children(self, dataseverse_id):\n",
        "        self.list_dataverse_children.append(dataseverse_id)\n",
        "\n",
        "    def remove_id_list_dataverse_children(self, dataseverse_id):\n",
        "        self.list_dataverse_children.remove(dataseverse_id)\n",
        "\n",
        "    def get_list_datasets(self):\n",
        "        return self.list_datasets\n",
        "\n",
        "    def get_list_dataverse_children(self):\n",
        "        return self.list_dataverse_children\n",
        "\n",
        "    def count(self):\n",
        "        return len(self.list_datasets)\n",
        "\n",
        "    def create_list_datasets(self, identifier):\n",
        "\n",
        "        conn = UtilsConnection(self.config)\n",
        "\n",
        "        url_api = f\"{self.config.get_api_url()}/api/dataverses/{identifier}/contents\"\n",
        "        object_json = conn.call_api(url_api, \"GET\")\n",
        "\n",
        "        if object_json:\n",
        "            self.config.get_logger().info(f\"Reading the API values\")\n",
        "            array_json = object_json.get(\"data\", {})\n",
        "\n",
        "            for value in array_json:\n",
        "                if value['type'] == 'dataverse':\n",
        "                    self.update_list_dataverse_children(value['id'])\n",
        "                elif value['type'] == 'dataset' and value['protocol'] == 'doi':\n",
        "                    self.update_list_dataset(value['protocol'] + ':' + value['authority'] + '/' + value['identifier'])\n",
        "        else:\n",
        "            self.config.get_logger().error(f\"Call API ERROR\")\n",
        "\n",
        "        if not identifier == self.identifier:\n",
        "            self.remove_id_list_dataverse_children(identifier)\n",
        "\n",
        "        if len(self.get_list_dataverse_children()) != 0:\n",
        "\n",
        "            self.create_list_datasets(self.get_list_dataverse_children()[0])\n",
        "def extract_value(data_dict):\n",
        "    \"\"\"\n",
        "    Function to extract type names and values from a JSON metadata dictionary.\n",
        "\n",
        "    Args:\n",
        "    data_dict (dict): JSON metadata dictionary.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Type names and values extracted from the metadata dictionary.\n",
        "    \"\"\"\n",
        "    if isinstance(data_dict, dict):\n",
        "        type_names = []\n",
        "        values = []\n",
        "        for key, value in data_dict.items():\n",
        "            if key == 'typeName' and 'value' in data_dict:\n",
        "                if isinstance(data_dict['value'], list):\n",
        "                    for v in data_dict['value']:\n",
        "                        type_names.append(data_dict['typeName'])\n",
        "                        values.append(v)\n",
        "                else:\n",
        "                    type_names.append(data_dict['typeName'])\n",
        "                    values.append(data_dict['value'])\n",
        "            elif isinstance(value, dict) and 'typeName' in value and 'value' in value:\n",
        "                type_names.append(value['typeName'])\n",
        "                values.append(value['value'])\n",
        "            elif isinstance(value, str) and key == 'typeName':\n",
        "                type_names.append(value)\n",
        "                values.append(value)\n",
        "            else:\n",
        "                extracted_type_names, extracted_values = extract_value(value)\n",
        "                type_names += extracted_type_names\n",
        "                values += extracted_values\n",
        "        return type_names, values\n",
        "    elif isinstance(data_dict, list):\n",
        "        type_names = []\n",
        "        values = []\n",
        "        for item in data_dict:\n",
        "            extracted_type_names, extracted_values = extract_value(item)\n",
        "            type_names += extracted_type_names\n",
        "            values += extracted_values\n",
        "        return type_names, values\n",
        "    else:\n",
        "        return [], []\n",
        "\n",
        "def export_metadata(base_url, token, doi, citation_keys, citation_values, customUAB_keys, customUAB_values, stateDataset):\n",
        "    \"\"\"\n",
        "    Function to export metadata from a dataset and store it in respective lists.\n",
        "\n",
        "    Args:\n",
        "    base_url (str): Base URL of the Dataverse repository.\n",
        "    token (str): API token for authentication.\n",
        "    doi (str): DOI of the dataset.\n",
        "    citation_keys (list): List to store citation metadata keys.\n",
        "    citation_values (list): List to store citation metadata values.\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    from pyDataverse.api import NativeApi, DataAccessApi\n",
        "    from pyDataverse.models import Dataverse\n",
        "    api = NativeApi(base_url, token)  # Function to access the API\n",
        "    data_api = DataAccessApi(base_url, token)  # Function to access data via the API\n",
        "    try:\n",
        "        dataset = api.get_dataset(doi)  # Retrieve dataset metadata\n",
        "\n",
        "\n",
        "      # Extract citation metadata if available\n",
        "        if 'citation' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
        "            metadata_citation = dataset.json()['data']['latestVersion']['metadataBlocks']['citation']['fields']\n",
        "            citation = extract_value(metadata_citation)\n",
        "            citation_keys.extend(citation[0])\n",
        "            citation_values.extend(citation[1])\n",
        "            for item in metadata_citation:\n",
        "                if isinstance(item['value'], str):\n",
        "                    index_change = citation_keys.index(item['typeName'])\n",
        "                    citation_values[index_change] = item['value']\n",
        "\n",
        "     # Extract Library UAB metadata if available\n",
        "\n",
        "        if 'customUAB' in dataset.json()['data']['latestVersion']['metadataBlocks']:\n",
        "            metadata_customUAB = dataset.json()['data']['latestVersion']['metadataBlocks']['customUAB']['fields']\n",
        "            customUAB = extract_value(metadata_customUAB)\n",
        "            customUAB_keys.extend(customUAB[0])\n",
        "            customUAB_values.extend(customUAB[1])\n",
        "            for item in metadata_customUAB:\n",
        "                if isinstance(item['value'], str):\n",
        "                    index_change = customUAB_keys.index(item['typeName'])\n",
        "                    customUAB_values[index_change] = item['value']\n",
        "\n",
        "    except KeyError or InvalidSchema:\n",
        "        print('S\\'ha produït un error en llegir les metadades del conjunt de dades: ' + doi)\n",
        "\n",
        "def extract_metadata(data, citation_keys, citation_values, customUAB_keys, customUAB_values):\n",
        "\n",
        "    for key, value in zip(citation_keys, citation_values):\n",
        "        if not isinstance(value, dict):\n",
        "            data.append([key, value])\n",
        "\n",
        "    for key, value in zip(customUAB_keys, customUAB_values):\n",
        "        if not isinstance(value, dict):\n",
        "            data.append([key, value])\n",
        "\n",
        "# Configuration and execution\n",
        "import logging\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Ask the user for the token\n",
        "token = input(\"Introduïu el vostre token i premeu enter: \")\n",
        "\n",
        "# Use the token in your configuration\n",
        "config = Config(api_url=\"https://dataverse.csuc.cat/\", logger=logger, token=token)\n",
        "\n",
        "# List of institutions\n",
        "institucions = [\n",
        "    'UB', 'UAB', 'UPC', 'UPF', 'UdG', 'UdL', 'URV', 'UOC', 'UVIC-UCC',\n",
        "    'URL', 'UIC', 'UIB', 'Agrotecnio', 'CED', 'CRAG', 'CREAF', 'CRM', 'CTFC',\n",
        "    'i2CAT', 'I3PT', 'IBEC', 'IBEI', 'ICAC-CERCA', 'ICFO-CERCA', 'ICN2',\n",
        "    'ICRA-CERCA', 'IDIBAPS', 'IDIBELL', 'IDIBGI-CERCA', 'IFAE', 'IJC',\n",
        "    'IPHES-CERCA', 'IRBBarcelona-CERCA', 'IRB', 'IRSICAIXA', 'IRTA',\n",
        "    'ISGLOBAL', 'VHIR'\n",
        "]\n",
        "# Add an option for selecting all\n",
        "options_institucions = ['Totes les institucions'] + institucions\n",
        "\n",
        "# Create widgets for instructions and selections\n",
        "instruction_text_institucions = widgets.HTML(\n",
        "    value=\"<b>Trieu una o més institucions:</b>\"\n",
        ")\n",
        "\n",
        "# Create the widget for multiple selection of institutions\n",
        "institucions_widget = widgets.SelectMultiple(\n",
        "    options=options_institucions,\n",
        "    value=[],\n",
        "    description='Institucions:',\n",
        "    disabled=False\n",
        ")\n",
        "# Function to save selected institutions and print them\n",
        "opcions = set()\n",
        "def save_selection_institucions(change):\n",
        "    global opcions\n",
        "    selected = set(change['new'])\n",
        "\n",
        "    if 'Totes les institucions' in selected:\n",
        "        opcions = set(institucions)  # Select all institutions if 'Totes les institucions' is chosen\n",
        "    else:\n",
        "        opcions.update(selected)  # Update the set with new selections\n",
        "\n",
        "    print(f\"Institucions: {list(opcions)}\")\n",
        "\n",
        "# Function to clear the selection and reset the institutions widget\n",
        "def restart_selection_institucions(button):\n",
        "    global opcions\n",
        "    institucions_widget.value = []  # Clear the selections in the widget\n",
        "    opcions.clear()  # Clear the global opcions set\n",
        "    print(\"La selecció de la institució s'ha restablert.\")\n",
        "\n",
        "# Observe changes in the widget selection for institutions\n",
        "institucions_widget.observe(save_selection_institucions, names='value')\n",
        "\n",
        "# Create a button to restart the selection for institutions\n",
        "restart_button_institucions = widgets.Button(description=\"Reiniciar la selecció d'institucions\")\n",
        "restart_button_institucions.on_click(restart_selection_institucions)\n",
        "\n",
        "# Display the widgets and button with instructions\n",
        "\n",
        "display(instruction_text_institucions, institucions_widget, restart_button_institucions)\n",
        "\n",
        "\n",
        "########################\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YwdSslz8k2Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Un cop escollides les institucions i les metadades, ciqueu el botó &#x25B6; per generar el taulell de metadades\n",
        "selected_metadata = [\"publicationRelationType\",\"publicationCitation\",\"publicationIDType\",\"publicationIDNumber\",\"publicationURL\",\"reviewLibrary\"]\n",
        "metadata_keys_list = []\n",
        "metadata_values_list = []\n",
        "list_doi = []\n",
        "instancia = []\n",
        "states = []\n",
        "for element in opcions:\n",
        "    processor = DatasetProcessor(config, element)\n",
        "    processor.create_list_datasets(element)\n",
        "    sigles=element\n",
        "    for i in processor.get_list_datasets():\n",
        "        metadata_keys_aux=[]\n",
        "        metadata_values_aux=[]\n",
        "        #  Metadata lists:\n",
        "        citation_keys, customUAB_keys, state = [[] for _ in range(3)]\n",
        "        citation_values, customUAB_values, state = [[] for _ in range(3)]\n",
        "        data = []\n",
        "        stateDataset = []\n",
        "        # Exporting metadata\n",
        "        export_metadata(config.get_api_url(), config.get_token(), i, citation_keys, citation_values, customUAB_keys, customUAB_values, stateDataset)\n",
        "        # Extracting metadata and arranging it\n",
        "        extract_metadata(data, citation_keys, citation_values, customUAB_keys, customUAB_values)\n",
        "        # Creating a DataFrame\n",
        "        df = pd.DataFrame(data, columns=['Metadata', 'Value'])\n",
        "        metadata_keys_aux = df['Metadata'].tolist()\n",
        "        metadata_values_aux = df['Value'].tolist()\n",
        "        metadata_keys_list.append(metadata_keys_aux)\n",
        "        metadata_values_list.append(metadata_values_aux)\n",
        "        instancia.append(sigles)\n",
        "        list_doi.append(i)\n",
        "        #states.append(stateDataset[0])\n",
        "def aggregate_metadata(metadata_keys_list, metadata_values_list, list_doi, selected_metadata):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Initialize dictionaries to store aggregated values\n",
        "    metadata_values = defaultdict(lambda: defaultdict(set))\n",
        "\n",
        "    # Aggregate values by DOI\n",
        "    for i in range(len(metadata_keys_list)):\n",
        "        doi = list_doi[i]\n",
        "        for key, value in zip(metadata_keys_list[i], metadata_values_list[i]):\n",
        "            if key in selected_metadata:\n",
        "                if isinstance(value, list):\n",
        "                    metadata_values[key][doi].update(value)\n",
        "                else:\n",
        "                    metadata_values[key][doi].add(value)\n",
        "\n",
        "    # Convert sets to sorted lists\n",
        "    aggregated_metadata = {field: [''] * len(list_doi) for field in selected_metadata}\n",
        "    for field in selected_metadata:\n",
        "        for doi in list_doi:\n",
        "            values = list(metadata_values[field][doi])\n",
        "            aggregated_metadata[field][list_doi.index(doi)] = '; '.join(values) if values else ''\n",
        "\n",
        "    return aggregated_metadata\n",
        "# Aggregate metadata values\n",
        "metadata = aggregate_metadata(metadata_keys_list, metadata_values_list, list_doi, selected_metadata)\n",
        "\n",
        "# Create the data dictionary\n",
        "data = {\n",
        "    'DOI': list_doi,\n",
        "    'Institution': instancia\n",
        "}\n",
        "\n",
        "# Add the dynamic metadata fields\n",
        "for field in selected_metadata:\n",
        "    data[field] = metadata[field]\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Extract the numeric part of the DOI and convert it to int for sorting\n",
        "df['DOI_Number'] = df['DOI'].str.extract(r'data(\\d+)').astype(int)\n",
        "\n",
        "# Sort the DataFrame based on the DOI_Number column\n",
        "df = df.sort_values(by='DOI_Number')\n",
        "\n",
        "# Format the DOI column as 'https://doi.org/10.34810/dataXXX'\n",
        "df['DOI'] = 'https://doi.org/10.34810/data' + df['DOI_Number'].astype(str)\n",
        "\n",
        "# Drop the DOI_Number column (optional)\n",
        "df = df.drop(columns=['DOI_Number'])\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BO6eCyxGJlpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clicar el botó &#x25B6; per guardar les dades en un fitxer excel.\n",
        "from google.colab import files\n",
        "# Save the DataFrame to an Excel file\n",
        "excel_filename = 'estudi_publicacio_relacionada.xlsx'\n",
        "df.to_excel(excel_filename, index=False)\n",
        "\n",
        "# Provide download link for the file\n",
        "files.download(excel_filename)"
      ],
      "metadata": {
        "id": "awN-hNkLqsxu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "outputId": "5322f439-f0cd-4c78-cc5b-7a7be713a415"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_593bd7fd-92cb-477a-935e-2065a4f40189\", \"estudi_publicacio_relacionada.xlsx\", 45103)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}